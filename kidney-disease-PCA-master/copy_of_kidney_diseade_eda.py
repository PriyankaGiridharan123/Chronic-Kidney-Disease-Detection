# -*- coding: utf-8 -*-
"""Copy_of_kidney_diseade_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lYFLKjEYlTxAjAKSADkxpeU7jrySwBKI
"""



"""# Kidney disease dataset EDA
# columns and attributes info          
            age		-	age
			bp		-	blood pressure
			sg		-	specific gravity
			al		-   	albumin
			su		-	sugar
			rbc		-	red blood cells
			pc		-	pus cell
			pcc		-	pus cell clumps
			ba		-	bacteria
			bgr		-	blood glucose random
			bu		-	blood urea
			sc		-	serum creatinine
			sod		-	sodium
			pot		-	potassium
			hemo		-	hemoglobin
			pcv		-	packed cell volume
			wc		-	white blood cell count
			rc		-	red blood cell count
			htn		-	hypertension
			dm		-	diabetes mellitus
			cad		-	coronary artery disease
			appet		-	appetite
			pe		-	pedal edema
			ane		-	anemia
			class		-	class

* 4.Number of Instances:  400 (250 CKD, 150 notckd)
%
* 5.Number of Attributes: 24 + class = 25 ( 11  numeric ,14  nominal)
%
* 6.Attribute Information :
 	>1.Age(numerical)
  	  	age in years
 	2.Blood Pressure(numerical)
	       	bp in mm/Hg
 	3.Specific Gravity(nominal)
	  	sg - (1.005,1.010,1.015,1.020,1.025)
 	4.Albumin(nominal)
		al - (0,1,2,3,4,5)
 	5.Sugar(nominal)
		su - (0,1,2,3,4,5)
 	6.Red Blood Cells(nominal)
		rbc - (normal,abnormal)
 	7.Pus Cell (nominal)
		pc - (normal,abnormal)
 	8.Pus Cell clumps(nominal)
		pcc - (present,notpresent)
 	9.Bacteria(nominal)
		ba  - (present,notpresent)
 	10.Blood Glucose Random(numerical)
		bgr in mgs/dl
 	11.Blood Urea(numerical)
		bu in mgs/dl
 	12.Serum Creatinine(numerical)
		sc in mgs/dl
 	13.Sodium(numerical)
		sod in mEq/L
 	14.Potassium(numerical)
		pot in mEq/L
 	15.Hemoglobin(numerical)
		hemo in gms
 	16.Packed  Cell Volume(numerical)
 	17.White Blood Cell Count(numerical)
		wc in cells/cumm
 	18.Red Blood Cell Count(numerical)
		rc in millions/cmm
 	19.Hypertension(nominal)
		htn - (yes,no)
 	20.Diabetes Mellitus(nominal)
		dm - (yes,no)
 	21.Coronary Artery Disease(nominal)
		cad - (yes,no)
 	22.Appetite(nominal)
		appet - (good,poor)
 	23.Pedal Edema(nominal)
		pe - (yes,no)
 	24.Anemia(nominal)
		ane - (yes,no)
 	25.Class (nominal)
		class - (ckd,notckd)

* 7. Missing Attribute Values: Yes(Denoted by "?")
%
* 8. Class Distribution: ( 2 classes)
    		Class 	  Number of instances
    		ckd          	  250
    		notckd       	  150   
     
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
#import assignment2_helper as helper

from sklearn.kernel_approximation import RBFSampler
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import classification_report
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
sns.set_style(style='whitegrid')
from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, classification_report,auc)
from sklearn.metrics import confusion_matrix



kidney=pd.read_csv('kidney_disease.csv')

def clas(x):
  if x=='ckd\t':
    return 'ckd'
  else:
    return x

kidney.classification=kidney.classification.apply(clas,convert_dtype=True)

kidney.classification.unique()
#print(kidney.classification)
#exit()

kidney.wc=pd.to_numeric(kidney.wc,errors='coerce')
kidney.rc=pd.to_numeric(kidney.rc,errors='coerce')
kidney.pcv=pd.to_numeric(kidney.pcv,errors='coerce')

"""# Count Plot of ckd vs notckd"""

sns.countplot(x='classification',data=kidney)
sns.FacetGrid(kidney,hue='classification',height=8) \
   .map(plt.scatter,'hemo','wc') \
   .add_legend()
#$plt.show()

sns.FacetGrid(kidney,hue='classification',height=8).map(plt.scatter,'hemo','rc').add_legend()
#$plt.show()

plt.figure(1,figsize=(20,8))
sns.scatterplot(x='bgr',y='wc',hue='classification',data=kidney,size='pcv')

def clas(x):
  if x=='\tno':
    return 'no'
  elif x=='yes':
    return 'yes'
  elif x=='no':
    return 'no'
  else:
    return 'unknown'

kidney.cad.unique()

kidney.cad=kidney.cad.apply(clas,convert_dtype=True)

plt.figure(1,figsize=(20,8))
plt.subplot(141)
sns.boxplot(x='classification',y='hemo',data=kidney)
plt.subplot(142)
sns.boxplot(x='classification',y='bgr',data=kidney)
plt.subplot(143)
sns.boxplot(x='classification',y='rc',data=kidney)
plt.subplot(144)
sns.boxplot(x='classification',y='wc',data=kidney)

#$plt.show()

plt.figure(1,figsize=(20,8))
plt.subplot(141)
sns.barplot(x='classification',y='hemo',data=kidney)
plt.subplot(142)
sns.barplot(x='classification',y='rc',data=kidney)
plt.subplot(143)
sns.barplot(x='classification',y='wc',data=kidney)
plt.subplot(144)
sns.barplot(x='classification',y='bgr',data=kidney)
#$plt.show()

sns.countplot(x='classification',data=kidney)

plt.figure(1,figsize=(20,8))
plt.subplot(141)
sns.violinplot(x='classification',y='hemo',data=kidney)
plt.subplot(142)
sns.violinplot(x='classification',y='rc',data=kidney)
plt.subplot(143)
sns.violinplot(x='classification',y='wc',data=kidney)
plt.subplot(144)
sns.violinplot(x='classification',y='bgr',data=kidney)
#$plt.show()

# { Blue = notckd  } and { red = ckd }"""

def col(x):
    if x=='ckd':
        return 'red'
    elif x=='notckd':
        return 'blue'

#kidney.classification=kidney.classification.apply(sal,convert_dtype=True)

#kidney.head()

sns.pairplot(kidney[['bgr','wc','rc','classification','hemo']],hue='classification',height=4)

sns.pairplot(kidney[['bgr','wc','rc','classification','hemo']],kind='reg',hue='classification',height=4,dropna=True)

kidney[kidney.classification=='ckd'][['bgr','wc','rc']].var()

kidney[kidney.classification=='notckd'][['bgr','wc','rc']].var()

kidney[kidney.classification=='ckd'][['bgr','wc','rc']].describe()

kidney[kidney.classification=='notckd'][['bgr','wc','rc']].describe()

kidney.shape[0]

kidney=kidney.dropna()

kidney.pcc.unique()

"""<center><h1>
Running PCA and training ML model to predict chronic and not chronic
</h1>
</center>
<center><img src="https://www.magellanic-clouds.com/blocks/wp-content/uploads/2017/01/ml_en.png"></center>
"""

kidney=kidney[['rc','wc','bgr','hemo','pcv','bu','sc','sod','pot','su','al','sg','bp','age','classification']]

#exit()
"""# Coorelation map of various factors"""

plt.figure(1,figsize=(20,10))
"""
#kidney.classification=pd.to_numeric(kidney.classification,errors='coerce')
sns.heatmap(kidney.corr(),annot=True,cmap='YlGnBu',linewidth=0.8)
#$plt.show()

scaleFeatures = False

labels = ['red' if i=='ckd' else 'green' for i in kidney.classification]


df = kidney[['bgr', 'wc', 'rc']]


df.head(10)

df.dtypes

print(df.var())
print("This is the describe output: ", df.describe())

plt.figure(1,figsize=(12,6))
sns.heatmap(df.corr(),annot=True,cmap='YlGnBu',linewidth=0.8)
"""
#$plt.show()

#if scaleFeatures: df = helper.scaleFeatures(df)


#from sklearn.decomposition import PCA
#pca = PCA(n_components = 2)
#pca.fit(df)
#T = pca.transform(df)


#plt.figure(1, figsize=(18,10))
#ax = helper.drawVectors(T, pca.components_, df.columns.values, plt, scaleFeatures)
"""
T = pd.DataFrame(T)
T.columns = ['component1', 'component2']
T.plot.scatter(x='component1', y='component2', marker='o', c=labels, alpha=0.75)#, ax=ax)
plt.show()
"""

"""# Scatter plot of PCA performed dataset
* As you can see Not chronic kidney diseases in green are easily separable from chronic kidney disease.

* Some results are mixed

# Although the dataset has reduced after cleaning but lets train it anyway.
"""

kidney.head()

def cl(x):
  if x=='ckd':
    return 1
  elif x=='notckd':
    return 0

kidney.classification=kidney.classification.apply(cl,convert_dtype=True)

#kidney.dtypes
print(kidney)
#exit()
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

clean_data=kidney.copy()

y=clean_data[['classification']].copy()

y.head()

kidney.columns

X = clean_data[['rc', 'wc', 'bgr', 'hemo', 'pcv', 'bu', 'sc', 'sod', 'pot', 'su', 'al',
       'sg', 'bp', 'age']].copy()

X.head()

X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,random_state=324)

print(type(X_train))
print(type(X_test))
print(type(y_train))
print(type(y_test))


y_train.describe()

X_train.head()

X_train.shape

y_train.shape


#================

kidney=kidney[['rc','wc','bgr','hemo','pcv','bu','sc','sod','pot','su','al','sg','bp','age','classification']]
feature_names=['rc','wc','bgr','hemo','pcv','bu','sc','sod','pot','su','al','sg','bp','age','classification']
kidney['rc'] = pd.to_numeric(kidney['rc'], errors='coerce') 
kidney['wc'] = pd.to_numeric(kidney['wc'], errors='coerce') 
kidney['bgr'] = pd.to_numeric(kidney['bgr'], errors='coerce') 
kidney['hemo'] = pd.to_numeric(kidney['hemo'], errors='coerce') 
kidney['pcv'] = pd.to_numeric(kidney['pcv'], errors='coerce') 
kidney['bu'] = pd.to_numeric(kidney['bu'], errors='coerce') 
kidney['sc'] = pd.to_numeric(kidney['sc'], errors='coerce') 
kidney['sod'] = pd.to_numeric(kidney['sod'], errors='coerce') 
kidney['pot'] = pd.to_numeric(kidney['pot'], errors='coerce') 
kidney['su'] = pd.to_numeric(kidney['su'], errors='coerce') 
kidney['al'] = pd.to_numeric(kidney['al'], errors='coerce') 
kidney['sg'] = pd.to_numeric(kidney['sg'], errors='coerce') 
kidney['bp'] = pd.to_numeric(kidney['bp'], errors='coerce') 
kidney['age'] = pd.to_numeric(kidney['age'], errors='coerce') 
#kidney['classification'] = pd.to_numeric(kidney['classification'], errors='coerce') 
kidney.fillna(0, inplace=True)
print(kidney.head())

data=kidney

# Importing necessary libraries
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Exclude 'id' and 'classification' from the features
X = data.drop(columns=['classification'])

# Standardize the data (important step before applying PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Applying PCA to reduce to 10 principal components
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_scaled)
#print(pd.DataFrame(pca.components_,columns=X.columns,index = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']))
#X_pc = model.transform(train_features)

# number of components
n_pcs= pca.components_.shape[0]

# get the index of the most important feature on EACH component i.e. largest absolute value
# using LIST COMPREHENSION HERE
most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]

initial_feature_names = feature_names

# get the names
most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]

# using LIST COMPREHENSION HERE AGAIN
dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}

# build the dataframe
#df = pd.DataFrame(sorted(dic.items()))
print(most_important_names)
most_important_names.append('classification')
kidney=kidney[most_important_names]
feature_names=[most_important_names]
print(kidney)
print(feature_names)

#=============
model = LogisticRegression()
traindata=kidney

y = traindata['classification']
indices =range(len(traindata))
X_train, X_test, y_train, y_test,tr,te = train_test_split(traindata, y, indices,test_size = 0.25, random_state = 42)

traindata=X_train
trainlabel= y_train#traindata.iloc[1:,11]
testlabel=y_test
print(trainlabel)
model.fit(traindata, trainlabel)

# make predictions
expected = testlabel
#np.savetxt('classical/expected.txt', expected, fmt='%01d')
predicted = model.predict(X_test)
proba = model.predict_proba(X_test)

#np.savetxt('classical/predictedlabelLR.txt', predicted, fmt='%01d')
#np.savetxt('classical/predictedprobaLR.txt', proba)

y_train1 = expected
y_pred = predicted
accuracy = accuracy_score(y_train1, y_pred)
recall = recall_score(y_train1, y_pred , average="binary")
precision = precision_score(y_train1, y_pred , average="binary")
f1 = f1_score(y_train1, y_pred, average="binary")

print("accuracy")
print("%.3f" %accuracy)
print("precision")
print("%.3f" %precision)
print("recall")
print("%.3f" %recall)
print("f1score")
print("%.3f" %f1)


def autolabel(rects,ax):
    """
    Attach a text label above each bar displaying its height
    """
    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,
                '%d' % int(height),
                ha='center', va='bottom')

Mat=confusion_matrix(y_train1, y_pred)
X=Mat.flatten(order='C')
Y=X#[ ['Item_Outlet_Sales']]
#values=X.groupby(['Item_Outlet_Sales'])['Item_Outlet_Sales'].count() #.plot.bar(figsize=(8, 6));
values=X
xaxistitles=['True Positive','True Negative','False Positive','False Negative']

ind = np.arange(4)#9)  # the x locations for the groups
width = 0.35   
men_std = (2, 2, 2, 2)#,2,2,2,2,2)
fig, ax = plt.subplots()
rects1 = ax.bar(ind, values, width, color='r', yerr=men_std)
 
ax.set_xlabel('CATEGORY')
ax.set_ylabel('RECORDS COUNT')
ax.set_title('CONFUSION MATRIX VALUES - LOGISTICS REGRESSION')
ax.set_xticks((ind + width / 2 ) )
ax.set_xticklabels(xaxistitles) #  ('1', '2', '3', '4', '5','6','7','8','9','10'))
 #ax.legend((rects1[0], rects2[0]), ('Men','Women'))
ax.legend(['Value'])
autolabel(rects1,ax)
 #autolabel(rects2)
#plt.savefig('FlaskDeployedApp/static/outputimages/prgsvm.png')
plt.show()


exit()
"""
kidney_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)
kidney_classifier.fit(X_train, y_train)

type(kidney_classifier)

predictions = kidney_classifier.predict(X_test)

predictions[:10]

y_test['classification'][:10]

accuracy_score(y_true = y_test, y_pred = predictions)
"""
"""# This is what happens when you have a small dataset and too many features.

* It's is here showing an accuracy score of **98%**.
"""








""" 
component_weights = pca.components_

# Create a mapping between component weights and feature names
feature_weights_mapping = {}
for i, component in enumerate(component_weights):
  component_feature_weights = zip(feature_names, component)
  sorted_feature_weight = sorted(
      component_feature_weights, key=lambda x: abs(x[1]), reverse=True)
  feature_weights_mapping[f"Component {i+1}"] = sorted_feature_weight
  
# Accessing feature names contributing to Principal Component
print("Feature names contributing to Principal Components")
for feature, weight in feature_weights_mapping.items():
  print(f"{feature}: {weight}") 
exit()
"""
"""
# Convert the reduced data into a DataFrame
pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(10)])

# Display the explained variance ratio to understand how much variance is captured by each component
print("Explained Variance Ratio by each Principal Component:")
print(pca.explained_variance_ratio_)

# Display the first few rows of the transformed data
print("\nPCA Reduced Data (First 5 rows):")
print(pca_df.head())
names =pca.components_
print(names)

exit()

kidney.head(8)
kidney.dtypes
"""



"""
ax=sns.jointplot('bgr','rc',data=kidney[kidney.classification=='ckd'],kind='kde',height=6,color='r')
ax=sns.jointplot('bgr','rc',data=kidney[kidney.classification=='notckd'],kind='kde',height=6,color='b')
ax=sns.jointplot('bgr','wc',data=kidney,kind='scatter',height=6,color='gold')
ax=sns.jointplot('bgr','wc',data=kidney[kidney.classification=='notckd'],kind='scatter',height=6,color='green')
ax=sns.jointplot('bgr','rc',data=kidney[kidney.classification=='ckd'],kind='hex',height=6,color='orange')
ax=sns.jointplot('bgr','rc',data=kidney[kidney.classification=='notckd'],kind='hex',height=6,color='yellow')

#fig = px.scatter_3d(kidney, x='rc', z='wc', y='bgr',color='classification',opacity=0.8)
#fig.show()

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure(1,figsize=(18,12))
ax = fig.add_subplot(111, projection='3d')
m=('^','o')
ax.scatter(kidney.rc, kidney.bgr, kidney.wc, c=kidney.classification, marker='o')

ax.set_xlabel('Red Blood Cells')
ax.set_ylabel('Blood Glucose Random')
ax.set_zlabel('White blood cells')

plt.show()
"""